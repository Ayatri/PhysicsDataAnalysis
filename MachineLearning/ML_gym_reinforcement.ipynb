{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_gym_reinforcement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iv2rJ1irpVc"
      },
      "source": [
        "# Cartpole game, as per https://keon.io/deep-q-learning/\n",
        "#!pip install gym\n",
        "\n",
        "# GYM provides an environment for an agent to interact with. We'll look at the 'cartpole',\n",
        "#  or the unstable, top-heavy inverted pendulum on top of a cart, which our 'agent' must learn to balance.\n",
        "#\n",
        "# GYM provides:\n",
        "#  - a state (observation) of the system/environment\n",
        "#    (in case of cartpole: cart position and velocity, angle and velocity at the tip\n",
        "#  - a set of _actions_ the agent can undertake in the environment\n",
        "#    (in case of cartpole: moving the cart base left or right)\n",
        "\n",
        "# When an agent performs a certain action in the current enviroment, GYM returns\n",
        "#  - a _reward_ for a certain result of performing the action in the environment\n",
        "#    (in case of cartpole: +1 if it didn't die.)\n",
        "#  - the new _state_ of the environment after performing said action\n",
        "#    (in case of cartpole: the effect of gravity pulling on the pendulum's top, on the angle, in one time step)\n",
        "#  - a 'terminal' in case the state is such that the agent cannot continue (is 'dead')\n",
        "#    (in case of cartpole: if the angle exceeds > 5 degrees of tipping over, or it flies off-screen)\n",
        "\n",
        "# It is up to us to make an agent that learns which action to take in which state, to maximise the total reward before it dies.\n",
        "#  We do this by 'remembering' what the results were of certain actions on certain states in a memory (i.e. a list or deque),\n",
        "#  And after every 'death' we sample ('replay') from this memory to train a neural network that decides the best actions\n",
        "#   we can take on any state we've encountered so far, which maximises the total reward.\n",
        "\n",
        "# In addition, GYM provides a nice real-time visual output of the environment and the result of actions taken.\n",
        "#  In google COLAB however, we pull some strings, and only have a combined video at the end.\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVf34L_Q0rE_"
      },
      "source": [
        "COLAB = True\n",
        "\n",
        "if(COLAB) :\n",
        "  # install some helpers to visualize gym graphics in the colab environment\n",
        "  !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg x11-utils > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA7nN0_rrwRa"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRnwGNBeuNbY"
      },
      "source": [
        "# only necessary for visualisation in colab env.\n",
        "if(COLAB) :\n",
        "  from IPython.display import HTML\n",
        "  from IPython import display as ipythondisplay\n",
        "  from pyvirtualdisplay import Display\n",
        "  from gym.wrappers import Monitor\n",
        "  import glob, io, os, base64\n",
        "  from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "  display = Display(visible=0, size=(1400, 900))\n",
        "  display.start()\n",
        "  #os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
        "\n",
        "  def combine_videos() :\n",
        "    mp4list = sorted( glob.glob('video/*.mp4') )\n",
        "    if len(mp4list) > 0 :\n",
        "      videolist = []\n",
        "      for mp4 in mp4list :\n",
        "        videolist.append( VideoFileClip(mp4))\n",
        "      final_clip = concatenate_videoclips(videolist)\n",
        "      final_clip.to_videofile(\"combined.mp4\", fps=24, remove_temp=False)\n",
        "      return 1\n",
        "    else :\n",
        "      return 0\n",
        "\n",
        "  def show_video() :\n",
        "      if(combine_videos()) :\n",
        "        video = io.open(\"combined.mp4\", 'r+b').read()\n",
        "\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                  loop controls style=\"height: 400px;\">\n",
        "                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "               </video>'''.format(encoded.decode('ascii'))))\n",
        "      else :\n",
        "        print(\"Error: no videos found\")     \n",
        "\n",
        "\n",
        "  def wrap_env(env):\n",
        "    env = Monitor(env, './video', video_callable=lambda episode_id: True, force=True)\n",
        "    return env\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVzZ7nmErzCw"
      },
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "MAXRUNS = 150\n",
        "#NSTEPSOLVED = 1000\n",
        "\n",
        "GAMMA = 0.95  # 'future discount factor'\n",
        "LEARNING_RATE = 0.001 # rate at which to update weights after each training step\n",
        "\n",
        "MEMORY_SIZE = 1000000 # size of container to hold actions and outcomes\n",
        "BATCH_SIZE = 20 # number of actions in memory to 'replay' after each death\n",
        "\n",
        "# probability to do 'random' actions, to sample from event space\n",
        "EXPLORATION_MAX = 1.0  \n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.995   # 0.995 --> 1% after ~35 runs\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2bZXXrqr5Tj"
      },
      "source": [
        "class DQNSolver:\n",
        "\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        self.exploration_rate = EXPLORATION_MAX\n",
        "\n",
        "        self.action_space = action_space\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "        # This will be a simple feed-forward NN, with \n",
        "        #  - input = 'observation' (aka state)\n",
        "        #  - output = predicted 'quality' of each possible action\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
        "        self.model.add(Dense(24, activation=\"relu\"))\n",
        "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
        "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
        "        self.model.summary()\n",
        "        \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # add event to memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        # return the best possible action for the current state\n",
        "\n",
        "        # sometimes allow for a random action at the 'exploration rate', to avoid local minima\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return random.randrange(self.action_space)\n",
        "        \n",
        "        # Get predicted qualities for each possible action, and return the action (=index) with the highest quality\n",
        "        q_values = self.model.predict(state) \n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def experience_replay(self):\n",
        "        # Learn from random subset of memory (reduces corr. between subsequent actions).\n",
        "        # learning is done by comparing 'predicted quality' to the here defined quality (~reward) of the action.\n",
        "        \n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            # We haven't experienced enough to properly learn yet - keep exploring!\n",
        "            return\n",
        "        \n",
        "        # Get random subset of memory\n",
        "        batch = random.sample(self.memory, BATCH_SIZE) \n",
        "        \n",
        "        for state, action, reward, state_next, terminal in batch:\n",
        "\n",
        "            # We define the 'quality' of a move by taking the known, memorized reward for the action,\n",
        "            #  and adding the predicted quality of the (predicted) best choice of action for the next state, to that.\n",
        "            # As the model learns to give this situation a low quality, any step leading up to this state will get a \n",
        "            #  lower quality due to the predict(state_next) term. This will slowly trickle through to the step before that, etc.,\n",
        "            #  slowly making our agent learn about future consequences of current actions.\n",
        "          \n",
        "            q_update = reward\n",
        "            if not terminal:\n",
        "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
        "                \n",
        "                # One could try to make the model learn from intermediate steps directly as well, speeding up the learning,\n",
        "                #  e.g. by changing the reward based on an increase in angle (although this should really be defined in the env.)\n",
        "                #q_update -= 1.0 * abs(state_next[0][2]) - abs(state[0][2]) # penalize angle increases for cartpole\n",
        "                \n",
        "            # - Define the quality of the non-chosen action to just be the predicted quality (i.e. diff = 0)\n",
        "            # - Define the quality of the chosen action to be the newly defined quality\n",
        "            q_values = self.model.predict(state)\n",
        "            q_values[0][action] = q_update \n",
        "            \n",
        "            # Finally, find the optimal model weights for minimal difference between \n",
        "            #  predicted quality and observed quality (+ future prediction as per above) for this action.\n",
        "            # The weights are then updated * learning rate\n",
        "            self.model.fit(state, q_values, verbose=0) \n",
        "            \n",
        "        # reduce the 'random choices' rate over time, because you expect the model to have learned\n",
        "        self.exploration_rate *= EXPLORATION_DECAY\n",
        "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCAlTnFAr8M8"
      },
      "source": [
        "def cartpole() :\n",
        "    if(COLAB) : env = wrap_env(gym.make(ENV_NAME)) # only for visualisation in colab\n",
        "    else : env = gym.make(ENV_NAME)\n",
        "\n",
        "    observation_space = env.observation_space.shape[0]\n",
        "    action_space = env.action_space.n\n",
        "    dqn_solver = DQNSolver(observation_space, action_space)\n",
        "       \n",
        "    run=0\n",
        "    runsteplog = []\n",
        "    #while True:\n",
        "    for i in range(MAXRUNS):\n",
        "        run += 1\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, observation_space])\n",
        "        step = 0\n",
        "        \n",
        "        while True :    \n",
        "            step += 1\n",
        "            screen = env.render() # graphical output\n",
        "\n",
        "            # decide on an action\n",
        "            #action = env.action_space.sample() # (this takes a random action)\n",
        "            action = dqn_solver.act(state) # takes best possible action from our agent\n",
        "            \n",
        "            # make the action\n",
        "            state_next, reward, terminal, info = env.step(action) \n",
        "\n",
        "            # if action made terminal: reduce reward!\n",
        "            reward = reward if not terminal else -reward  \n",
        "\n",
        "            state_next = np.reshape(state_next, [1, observation_space])\n",
        "\n",
        "            # fill agent memory with this action's results\n",
        "            dqn_solver.remember(state, action, reward, state_next, terminal) \n",
        "\n",
        "            # prepare for the next action in the environment\n",
        "            state = state_next \n",
        "            \n",
        "            if terminal :\n",
        "                #state = env.reset()\n",
        "                #state = np.reshape(state, [1, observation_space])\n",
        "                print(\"Run: {0}, exploration: {1:.15f}, score: {2}\".format(run,dqn_solver.exploration_rate,step))\n",
        "                runsteplog += [step]\n",
        "\n",
        "                break\n",
        "            \n",
        "            dqn_solver.experience_replay() # learn from batch of memories every time a new one is made\n",
        "            \n",
        "            #if(step > NSTEPSOLVED) :\n",
        "            #    i = MAXRUNS\n",
        "            #    print(\"Solved! (step > NSTEPSOLVED)\")\n",
        "            #    break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "    \n",
        "\n",
        "\n",
        "    # Show training process\n",
        "    \n",
        "    # plot #steps achieved\n",
        "    plt.plot(runsteplog)\n",
        "    plt.ylabel(\"# actions before terminal\")\n",
        "    plt.xlabel(\"run iteration\")\n",
        "\n",
        "    success_measure = np.mean(runsteplog[-15:])\n",
        "    print(\"Mean of last 15 runs: {0}\".format(success_measure))\n",
        "    return success_measure"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBDY7DTDr-uK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "97bb47dd-3d38-422d-e82b-0735d0dea627"
      },
      "source": [
        "MAXRUNS = 30\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cartpole()\n",
        "\n",
        "if(COLAB) :\n",
        "    show_video() "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 24)                120       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Run: 1, exploration: 0.913724886012593, score: 38\n",
            "Run: 2, exploration: 0.869052995545260, score: 11\n",
            "Run: 3, exploration: 0.790104972547028, score: 20\n",
            "Run: 4, exploration: 0.747719459303254, score: 12\n",
            "Run: 5, exploration: 0.518589330948458, score: 74\n",
            "Run: 6, exploration: 0.485873963736318, score: 14\n",
            "Run: 7, exploration: 0.464441858330825, score: 10\n",
            "Run: 8, exploration: 0.415948086273354, score: 23\n",
            "Run: 9, exploration: 0.331953891352235, score: 46\n",
            "Run: 10, exploration: 0.311012478166536, score: 14\n",
            "Run: 11, exploration: 0.291392160463186, score: 14\n",
            "Run: 12, exploration: 0.274381504048190, score: 13\n",
            "Run: 13, exploration: 0.246967342234727, score: 22\n",
            "Run: 14, exploration: 0.214627708570941, score: 29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b3894898871b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcartpole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOLAB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-29e192fded1a>\u001b[0m in \u001b[0;36mcartpole\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mdqn_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# learn from batch of memories every time a new one is made\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m#if(step > NSTEPSOLVED) :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f6fa5a483937>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#  predicted quality and observed quality (+ future prediction as per above) for this action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# The weights are then updated * learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# reduce the 'random choices' rate over time, because you expect the model to have learned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mprefetch\u001b[0;34m(self, buffer_size)\u001b[0m\n\u001b[1;32m   1165\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \"\"\"\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPrefetchDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, buffer_size, slack_period)\u001b[0m\n\u001b[1;32m   4475\u001b[0m           \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4476\u001b[0m           \u001b[0mslack_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslack_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4477\u001b[0;31m           **self._flat_structure)\n\u001b[0m\u001b[1;32m   4478\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrefetchDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mprefetch_dataset\u001b[0;34m(input_dataset, buffer_size, output_types, output_shapes, slack_period, legacy_autotune, buffer_size_min, name)\u001b[0m\n\u001b[1;32m   5288\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;34m\"slack_period\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslack_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"legacy_autotune\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegacy_autotune\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5290\u001b[0;31m         \"buffer_size_min\", buffer_size_min)\n\u001b[0m\u001b[1;32m   5291\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5292\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecN2oFiWsAkH"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}